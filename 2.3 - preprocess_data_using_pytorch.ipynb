{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2db48e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Zhengxiang (Jack) Wang \n",
    "# Date: 2022-01-21\n",
    "# GitHub: https://github.com/jaaack-wang "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423b97cc",
   "metadata": {},
   "source": [
    "## Quick start\n",
    "\n",
    "With wrapped up functions that we will gradually learn throughout this tutorial, preprocessing the text data into one that is ready for model training can be as simple as following. Does it really work? Let's explore!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84f29b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/w9/d_nplhzj4qx35xxlgljgdtjh0000gn/T/jieba.cache\n",
      "Loading model cost 0.649 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two vocabulary dictionaries have been built!\n",
      "Please call \u001b[1mX.vocab_to_idx | X.idx_to_vocab\u001b[0m to find out more where [X] stands for the name you used for this TextVectorizer class.\n"
     ]
    }
   ],
   "source": [
    "from utils import load_dataset, gather_text\n",
    "from pytorch_utils import * \n",
    "\n",
    "train_set = load_dataset('train.tsv')\n",
    "\n",
    "text = gather_text(train_set)\n",
    "V = TextVectorizer()\n",
    "V.build_vocab(text)\n",
    "\n",
    "batchify_fn = get_batchify_fn(V, include_seq_len=False)\n",
    "train_loader = create_dataloader(train_set, batchify_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebf68a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[1524,  107,  385,  ...,    0,    0,    0],\n",
      "        [ 141,  191,   68,  ...,    0,    0,    0],\n",
      "        [1034,    8,   25,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [  76,  391,  109,  ...,    0,    0,    0],\n",
      "        [ 293,   13, 1054,  ...,    0,    0,    0],\n",
      "        [1247,   41,   46,  ...,    0,    0,    0]]), tensor([0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
      "        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,\n",
      "        0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0]))\n"
     ]
    }
   ],
   "source": [
    "for example in train_loader:\n",
    "    print(example)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e704c8e6",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this tutorial, we will use functions from `pytorch` to help us preprocess and numericalize datasets. As these functions are native to `pytorch`, so there is an advantage in training models constructed by using `pytorch`, especially when the datasets are large. You will also need [`torchtext`](https://github.com/pytorch/text), a nlp package designed by the `paddle` team, to get everything going. To download it, simply run in command `pip3 install torchtext`.\n",
    "\n",
    "If you need more intuition about the ins and outs of this process, please refer to `2 - preprocess_data.ipynb` in the same folder.\n",
    "\n",
    "Below are the structure of this tutoiral:\n",
    "\n",
    "- [Load dataset](#1)\n",
    "- [Create vocab_to_idx mapping dictionary](#2)\n",
    "- [Text encoder](#3)\n",
    "- [Creating dataloader](#5)\n",
    "    - [Transform the dataset into  Dataset class using MapDataset](#5-1)\n",
    "    - [Building a batchify method](#5-3)\n",
    "    - [Now the dataloader](#5-4)\n",
    "- [A quick test](#6)\n",
    "- [Wrapped up functions](#7)\n",
    "    - [TextVectorizer](#7-1)\n",
    "    - [Get batchify_fn](#7-4)\n",
    "    - [Create dataloader](#7-5)\n",
    "- [More thorough tests](#8)\n",
    "    - [Initializations](#8-1)\n",
    "    - [Test One: CNN](#8-2)\n",
    "    - [Test Two: RNN](#8-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0077be",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "## Load dataset\n",
    "\n",
    "As usual, let's first use the `load_dataset` function compiled in the last two tutorials to load the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3616160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import load_dataset\n",
    "\n",
    "train_set = load_dataset('train.tsv')\n",
    "\n",
    "# check. should be 3000 (recall `1 - get_data.ipynb`)\n",
    "len(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd1e127",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "\n",
    "## Create `vocab_to_idx` mapping dictionary\n",
    "\n",
    "The purpose of creating a `vocab_to_idx` mapping dictionary is for later encoding or numeralizing text data for model training. In the `2.1 - wrapped_up_data_preprocessor` tutorial, we have learnt how to use `TextVectorizer` to conveniently do this job. \n",
    "\n",
    "In this tutorial, we will use `torchtext.vocab.build_vocab_from_iterator` to do a simlar job. For the tokenizer, we will still use `jieba` to tokenize Chinese. Alternatively, you can just use the `.split` function to tokenize English or use the `tokenize` function from `utils.py`. \n",
    "\n",
    "`torchtext.vocab.build_vocab_from_iterator` is indeed a long path to import and the essential functions that come with it can also be found in the `TextVectorizer` with more intuitive names, but we will deal with it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ceb66d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1adc7b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we need a tokenize func\n",
    "tokenize = jieba.lcut\n",
    "\n",
    "# Then we need a list of tokenized texts\n",
    "from utils import gather_text\n",
    "text = gather_text(train_set) # ---> gather text from the train_set\n",
    "tokens = list(map(tokenize, text)) # ---> a list of tokenized texts ([[w1, w2...], [w1, w2...]...])\n",
    "\n",
    "# build the vocabulary which will give us the mapping dictionaries for encoding\n",
    "# the order of the inputs for the \"specials\" matters. The first item will be indexed as 0, the second 1, and so on..\n",
    "V = build_vocab_from_iterator(tokens, specials=['[PAD]', '[UNK]'])\n",
    "V.set_default_index(V['[UNK]']) # ---> This must be set to represent all unseen tokens that may occur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a7cb67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 10 examples from the V.get_itos() LIST\n",
      "\n",
      "[PAD]               0\n",
      "[UNK]               1\n",
      "，                   2\n",
      "的                   3\n",
      "。                   4\n",
      "了                   5\n",
      "                    6\n",
      "是                   7\n",
      "我                   8\n",
      ",                   9\n",
      "\n",
      "\n",
      "The first 10 examples from the V.get_stoi() DICTIONARY\n",
      "\n",
      "22092               Ｋ\n",
      "22091               Ｂ\n",
      "22090               Ａ\n",
      "22089               ＠\n",
      "22086               龙应台\n",
      "22085               龙城\n",
      "22084               龙之梦\n",
      "22082               龌龊\n",
      "22079               鼻炎\n",
      "22078               鼻涕\n"
     ]
    }
   ],
   "source": [
    "# check\n",
    "\n",
    "tmp = \"{:20}{}\"\n",
    "print(\"The first 10 examples from the V.get_itos() LIST\\n\")\n",
    "\n",
    "for idx, tk in enumerate(V.get_itos()[:10]): # \"itos\" --> index to str, a list of str is output\n",
    "    print(tmp.format(tk, str(idx)))\n",
    "    \n",
    "    \n",
    "print(\"\\n\\nThe first 10 examples from the V.get_stoi() DICTIONARY\\n\")\n",
    "\n",
    "for tk, idx in list(V.get_stoi().items())[:10]: # \"stoi\" --> str to idx, a dictionary (str:idx) is output\n",
    "    print(tmp.format(str(idx), tk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5bd4b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index for me: 640\n",
      "Index for \u001b[1mThis_Word_Does_Not_Exist\u001b[0m: 1\n"
     ]
    }
   ],
   "source": [
    "# To look up indice for (a) token(s), use V.lookup_indices\n",
    "\n",
    "me_idx = V.lookup_indices(['今天']) # the input must be in a list!\n",
    "print(\"Index for me:\", me_idx[0]) # the output is also a list!\n",
    "\n",
    "unk_idx = V.lookup_indices(['This_Word_Does_Not_Exist'])\n",
    "print(\"Index for \\033[1mThis_Word_Does_Not_Exist\\033[0m:\", unk_idx[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff694234",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a>\n",
    "## Text encoder\n",
    "\n",
    "With this `V.lookup_indices` method, we do not have to write a for loop ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e9f2b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_encoder(text, \n",
    "                 tokenize=tokenize, \n",
    "                 idx_lookup=V.lookup_indices):\n",
    "    \n",
    "    tokens = tokenize(text)\n",
    "    return idx_lookup(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1af9d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: 选择珠江花园的原因就是方便，有电动扶梯直接到达海边，周围餐馆、食廊、商场、超市、摊位一应俱全。酒店装修一般，但还算整洁。 泳池在大堂的屋顶，因此很小，不过女儿倒是喜欢。 包的早餐是西式的，还算丰富。 服务吗，一般\n",
      "Encoded text: [189, 9545, 1205, 3, 429, 37, 99, 2, 17, 18740, 16391, 348, 917, 1944, 2, 606, 2482, 27, 21892, 27, 2185, 27, 1209, 27, 16748, 5476, 4, 13, 291, 76, 2, 44, 21, 260, 1071, 4, 6, 2870, 14, 284, 3, 4818, 2, 1102, 427, 2, 96, 399, 497, 7, 59, 4, 6, 533, 3, 108, 7, 2920, 3, 2, 21, 260, 472, 4, 6, 43, 320, 2, 76]\n"
     ]
    }
   ],
   "source": [
    "# check\n",
    "\n",
    "print(\"Original text:\", text[0])\n",
    "print(\"Encoded text:\", text_encoder(text[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a58376c",
   "metadata": {},
   "source": [
    "<a name=\"5\"></a>\n",
    "## Creating dataloader\n",
    "\n",
    "Now comes with the most important points! **I figure that detailed explanations may not help you to understand what will be shown below, because you may need to practice again and again, and compare with what we have done previously to build a solid intuition.** Let's simply take a dataloder as a black box. All you need to know is what needs to go in and what will come out. Here are some of the points you need to know:\n",
    "\n",
    "- A dataloader is something iterable and will work more efficiently with the models constructed by a deep learning framework, especially when trained on GPUs because they can load data asynchronously.\n",
    "\n",
    "\n",
    "- For a dataloader, it usually comes with parameters like these (besides a dataset that must be passed): a `sampler` method that create samples (batches) from the given dataset and return indices relating to examples from the dataset; a `collate_fn` method that further preprocess the batched examples. Alternatively, instead of passing a `sampler` to the dataloader, we can specify a value to the `batch_size` directly. We will do the later here. More about dataloader, please refer to [here](https://pytorch.org/docs/stable/data.html).\n",
    "\n",
    "\n",
    "- For the dataset, its type needs to be what is called `Dataset` (map-style dataset) or `IterableDataset` (iterable-style dataset) in order to make everything work. \n",
    "\n",
    "\n",
    "Enough words. Let's just see what this can be done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe09ae3",
   "metadata": {},
   "source": [
    "<a name=\"5-1\"></a>\n",
    "### Transform the dataset into  `Dataset` class \n",
    "\n",
    "A key property of the `Dataset`, It is iterable both by a for loop and by a slicing index (just like a list!) \n",
    "\n",
    "Here, we will use `torchtext.data.functional.to_map_style_dataset` to do the transformation. I know the import path suck! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73e18a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of train <class 'torchtext.data.functional.to_map_style_dataset.<locals>._MapStyleDataset'>\n",
      "Is train's type a Dataset? True\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "train = to_map_style_dataset(train_set)\n",
    "\n",
    "print(\"Type of train\", type(train))\n",
    "print(\"Is train's type a Dataset?\", isinstance(train, torch.utils.data.Dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f345bd18",
   "metadata": {},
   "source": [
    "<a name=\"5-3\"></a>\n",
    "### Building a `batchify` method \n",
    "\n",
    "The purpose of the `batchify` method is to provide a set of methods to further preprocess the bacthed dataset in a way that make possible model training. More concretely, as the batched dataset is still raw text plus label in our case, we will need to do the following:\n",
    "\n",
    "- first, we need to encode or numericalize the text data using the `text_encoder` we build above; \n",
    "- second, we need to make sure that the text ids (numericalized text) within a batch and of same kind (text_a versus text_b) are of same length/dimension (aligned with the max length in a batch or a `max_seq_len`);\n",
    "- then, for every bacthed element (e.g., text_a, text_b, label), we want them to be separated. \n",
    "- finally, for those RNN models, we will need to ensure that the outputs also include the \"text_seq_len\" info. \n",
    "\n",
    "This built `batchify` method will be passed to the `collate_fn` argument in the dataloader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7df60525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify_fn(batch, \n",
    "                text_encoder=text_encoder, \n",
    "                pad_idx=0,\n",
    "                max_seq_len=None, \n",
    "                include_seq_len=False, \n",
    "                dtype=torch.int64):\n",
    "    \n",
    "    # ----- pad func for a list -----\n",
    "    def _pad(lst, max_len):\n",
    "        dif = max_len - len(lst)\n",
    "        if dif > 0:\n",
    "            return lst + [pad_idx] * dif\n",
    "        if dif < 0:\n",
    "            return lst[:max_len]\n",
    "        return lst\n",
    "    \n",
    "    # ----- pad func for a bacth of lists -----\n",
    "    def pad(data):\n",
    "        if max_seq_len:\n",
    "            max_len = max_seq_len\n",
    "        else:\n",
    "            max_len = max(len(d) for d in data)\n",
    "        \n",
    "        for i, d in enumerate(data):\n",
    "            data[i] = _pad(d, max_len)\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    # ----- turn a list of int into tensor -----\n",
    "    def to_tensor(x):\n",
    "        return torch.tensor(x, dtype=dtype)\n",
    "    \n",
    "    \n",
    "    # ----- start batchifying -----\n",
    "    out_t, out_l = [], []\n",
    "    \n",
    "    for (text, label) in batch:\n",
    "        out_t.append(text_encoder(text)) \n",
    "        out_l.append(label)\n",
    "    \n",
    "    if include_seq_len:\n",
    "    \n",
    "    # if include_seq_len and max_seq_len, longer text will be trimmed\n",
    "    # hence, their text_seq_len is also reduced to the max_seq_len\n",
    "        if max_seq_len:\n",
    "            t_len = to_tensor([len(t) if len(t) < max_seq_len \n",
    "                               else max_seq_len for t in out_t])\n",
    "        else:\n",
    "            t_len = to_tensor([len(t) for t in out_t])\n",
    "        \n",
    "    # torch.cat put a list of tensor together\n",
    "    out_t = to_tensor(pad(out_t))\n",
    "    out_l = to_tensor(out_l)\n",
    "    \n",
    "    if include_seq_len:\n",
    "        return out_t, t_len, out_l\n",
    "    \n",
    "    return out_t, out_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e9618e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of text_ids preprocessed: torch.Size([4000, 928])\n",
      "Shape of text_len preprocessed: torch.Size([4000])\n",
      "Shape of labels preprocessed: torch.Size([4000])\n"
     ]
    }
   ],
   "source": [
    "# check. Note the following has not been batched.\n",
    "\n",
    "t, t_l, l = batchify_fn(train_set, include_seq_len=True)\n",
    "print(\"Shape of text_ids preprocessed:\", t.shape)\n",
    "print(\"Shape of text_len preprocessed:\", t_l.shape)\n",
    "print(\"Shape of labels preprocessed:\", l.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4c81c0",
   "metadata": {},
   "source": [
    "<a name=\"5-4\"></a>\n",
    "### Now the dataloader\n",
    "\n",
    "We will call the `torch.utils.data.DataLoader` and then set the \"batch_size\" as well as whether to shuffle the dataset passed to the dataloader. The output cannot be retrieved by index or is not subscriptable. \n",
    "\n",
    "**Please note that**, the `batchify_fn` passed to the `DataLoader` can only take one batch of text data as input. However, although our `batchify_fn` can take multiple values, except the `batch` parameter, other parameters all have a default values when not given. Neverthelss, in this senario, if we want to set `include_seq_len=True` or change other parameters, we will need to change the default values directly every time we run the `DataLoader`. We will introduce a `get_batchify_fn` method later to eliminate this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7419629",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    train,\n",
    "    batch_size=64,\n",
    "    shuffle = True, \n",
    "    collate_fn=batchify_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f67ad149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[   13,   305,    27,  ...,     0,     0,     0],\n",
      "        [  401,    24,    35,  ...,     0,     0,     0],\n",
      "        [21366,    60,   950,  ...,   780,    78,    61],\n",
      "        ...,\n",
      "        [    8,   153,  1803,  ...,     0,     0,     0],\n",
      "        [  229,   113,   148,  ...,     0,     0,     0],\n",
      "        [  750,     7,    43,  ...,     0,     0,     0]]), tensor([1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,\n",
      "        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0]))\n"
     ]
    }
   ],
   "source": [
    "# check\n",
    "\n",
    "for d in dataloader:\n",
    "    print(d)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff29605",
   "metadata": {},
   "source": [
    "<a name=\"6\"></a>\n",
    "## A quick test\n",
    "\n",
    "It works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58befa4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 {'Train loss': '0.92213', 'Train accu': '28.70'}\n",
      "\n",
      "Epoch 2/2 {'Train loss': '0.63757', 'Train accu': '49.53'}\n",
      "\n",
      "CPU times: user 7.05 s, sys: 298 ms, total: 7.35 s\n",
      "Wall time: 5.34 s\n"
     ]
    }
   ],
   "source": [
    "from pytorch_utils import PyTorchUtils\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from pytorch_models.BoW import BoW\n",
    "\n",
    "\n",
    "model = BoW(len(V), 2)\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "PT = PyTorchUtils(model, optimizer, criterion, include_seq_len=False)\n",
    "%time PT.train(dataloader, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c522aaac",
   "metadata": {},
   "source": [
    "<a name=\"7\"></a>\n",
    "## Wrapped up functions\n",
    "\n",
    "Before heading to the next section, you can test the following functions/class methods up and see if you can utilize them to do a quick start yourself!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e56762e",
   "metadata": {},
   "source": [
    "<a name=\"7-1\"></a>\n",
    "### TextVectorizer\n",
    "\n",
    "The following wrapped up class method remsembles the one that we built in the `2.1 - wrapped_up_data_preprocessor.ipynb`, but here we are using as many functions as from `paddle`. If you are interested, you can look a look back at the `TextVectorizer` inside the `utils.py` and see if you can create some additional functions (such as save the results into json file for later re-loading). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c447ef33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from collections import defaultdict\n",
    "from collections.abc import Iterable\n",
    "\n",
    "\n",
    "class TextVectorizer:\n",
    "     \n",
    "    def __init__(self, tokenizer=None):\n",
    "        self.tokenize = tokenizer if tokenizer else jieba.lcut\n",
    "        self.vocab_to_idx = {}\n",
    "        self.idx_to_vocab = {}\n",
    "        self._V = None\n",
    "    \n",
    "    def build_vocab(self, text):\n",
    "        tokens = list(map(self.tokenize, text))\n",
    "        \n",
    "        self._V = build_vocab_from_iterator(tokens, specials=['[PAD]', '[UNK]'])\n",
    "        for idx, tk in enumerate(self._V.get_itos()):\n",
    "            self.vocab_to_idx[tk] = idx\n",
    "            self.idx_to_vocab[idx] = tk\n",
    "        \n",
    "        self.vocab_to_idx = defaultdict(lambda: self.vocab_to_idx['[UNK]'], \n",
    "                                        self.vocab_to_idx)\n",
    "        \n",
    "        print('Two vocabulary dictionaries have been built!\\n' \\\n",
    "             + 'Please call \\033[1mX.vocab_to_idx | X.idx_to_vocab\\033[0m to find out more' \\\n",
    "             + ' where [X] stands for the name you used for this TextVectorizer class.')\n",
    "        \n",
    "    def text_encoder(self, text):\n",
    "        if isinstance(text, list):\n",
    "            return [self(t) for t in text]\n",
    "        \n",
    "        tks = self.tokenize(text)\n",
    "        out = [self.vocab_to_idx[tk] for tk in tks]\n",
    "        return out\n",
    "            \n",
    "    def text_decoder(self, text_ids, sep=\" \"):\n",
    "        if all(isinstance(ids, Iterable) for ids in text_ids):\n",
    "            return [self.text_decoder(ids, sep) for ids in text_ids]\n",
    "            \n",
    "        out = []\n",
    "        for text_id in text_ids:\n",
    "            out.append(self.idx_to_vocab[text_id])\n",
    "            \n",
    "        return f'{sep}'.join(out)\n",
    "    \n",
    "    def __call__(self, text):\n",
    "        if self.vocab_to_idx:\n",
    "            return self.text_encoder(text)\n",
    "        raise ValueError(\"No vocab is built!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e693dd1",
   "metadata": {},
   "source": [
    "<a name=\"7-4\"></a>\n",
    "### Get batchify_fn\n",
    "\n",
    "We will not change the `batchify_fn` we already built except a slight change of name, but we will customize a method to return batchify_fn for us on top of that for this series of tutorial!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f2609b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def _batchify_fn(batch, \n",
    "                 text_encoder, \n",
    "                 pad_idx=0,\n",
    "                 max_seq_len=None, \n",
    "                 include_seq_len=False, \n",
    "                 dtype=torch.int64):\n",
    "    \n",
    "    # ----- pad func for a list -----\n",
    "    def _pad(lst, max_len):\n",
    "        dif = max_len - len(lst)\n",
    "        if dif > 0:\n",
    "            return lst + [pad_idx] * dif\n",
    "        if dif < 0:\n",
    "            return lst[:max_len]\n",
    "        return lst\n",
    "    \n",
    "    # ----- pad func for a bacth of lists -----\n",
    "    def pad(data):\n",
    "        if max_seq_len:\n",
    "            max_len = max_seq_len\n",
    "        else:\n",
    "            max_len = max(len(d) for d in data)\n",
    "        \n",
    "        for i, d in enumerate(data):\n",
    "            data[i] = _pad(d, max_len)\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    # ----- turn a list of int into tensor -----\n",
    "    def to_tensor(x):\n",
    "        return torch.tensor(x, dtype=dtype)\n",
    "    \n",
    "    \n",
    "    # ----- start batchifying -----\n",
    "    out_t, out_l = [], []\n",
    "    \n",
    "    for (text, label) in batch:\n",
    "        out_t.append(text_encoder(text)) \n",
    "        out_l.append(label)\n",
    "    \n",
    "    if include_seq_len:\n",
    "    \n",
    "    # if include_seq_len and max_seq_len, longer text will be trimmed\n",
    "    # hence, their text_seq_len is also reduced to the max_seq_len\n",
    "        if max_seq_len:\n",
    "            t_len = to_tensor([len(t) if len(t) < max_seq_len \n",
    "                               else max_seq_len for t in out_t])\n",
    "        else:\n",
    "            t_len = to_tensor([len(t) for t in out_t])\n",
    "        \n",
    "    # torch.cat put a list of tensor together\n",
    "    out_t = to_tensor(pad(out_t))\n",
    "    out_l = to_tensor(out_l)\n",
    "    \n",
    "    if include_seq_len:\n",
    "        return out_t, t_len, out_l\n",
    "    \n",
    "    return out_t, out_l\n",
    "\n",
    "\n",
    "def get_batchify_fn(text_encoder, \n",
    "                    pad_idx=0,\n",
    "                    max_seq_len=None, \n",
    "                    include_seq_len=False, \n",
    "                    dtype=torch.int64):\n",
    "    \n",
    "    return lambda ex: _batchify_fn(ex, text_encoder, \n",
    "                                   pad_idx, max_seq_len, \n",
    "                                   include_seq_len, dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68e897c",
   "metadata": {},
   "source": [
    "<a name=\"7-5\"></a>\n",
    "### Create dataloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4fd4631a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "def create_dataloader(dataset, \n",
    "                      batchify_fn, \n",
    "                      batch_size=64, \n",
    "                      shuffle=True):\n",
    "    \n",
    "    \n",
    "    if not isinstance(dataset, Dataset):\n",
    "        dataset = to_map_style_dataset(dataset)\n",
    "        \n",
    "    \n",
    "    dataloder = DataLoader(dataset, \n",
    "                           batch_size=batch_size, \n",
    "                           shuffle=shuffle,\n",
    "                           collate_fn=batchify_fn)\n",
    "    \n",
    "    return dataloder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5e3363",
   "metadata": {},
   "source": [
    "<a name=\"8\"></a>\n",
    "## More thorough tests \n",
    "\n",
    "This time, we will include the dev_set for validation and the test_set for evaluation!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450e0a12",
   "metadata": {},
   "source": [
    "<a name=\"8-1\"></a>\n",
    "### Initializations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f2be1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two vocabulary dictionaries have been built!\n",
      "Please call \u001b[1mX.vocab_to_idx | X.idx_to_vocab\u001b[0m to find out more where [X] stands for the name you used for this TextVectorizer class.\n"
     ]
    }
   ],
   "source": [
    "from utils import load_dataset, gather_text\n",
    "\n",
    "train_set, dev_set, test_set = load_dataset(['train.tsv', 'dev.tsv', 'test.tsv'])\n",
    "\n",
    "text = gather_text(train_set)\n",
    "V = TextVectorizer()\n",
    "V.build_vocab(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71a018c",
   "metadata": {},
   "source": [
    "<a name=\"8-2\"></a>\n",
    "### Test One: CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4301a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchify_fn = get_batchify_fn(V, include_seq_len=False)\n",
    "train_loader = create_dataloader(train_set, batchify_fn)\n",
    "dev_loader = create_dataloader(dev_set, batchify_fn, shuffle=False)\n",
    "test_loader = create_dataloader(test_set, batchify_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9432896d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 {'Train loss': '0.68719', 'Train accu': '45.78'}\n",
      "Validation... {'Dev loss': '0.65926', 'Dev accu': '55.62'}\n",
      "\n",
      "Epoch 2/5 {'Train loss': '0.58850', 'Train accu': '70.54'}\n",
      "Validation... {'Dev loss': '0.53728', 'Dev accu': '70.78'}\n",
      "\n",
      "Epoch 3/5 {'Train loss': '0.45382', 'Train accu': '79.09'}\n",
      "Validation... {'Dev loss': '0.47383', 'Dev accu': '75.25'}\n",
      "\n",
      "Epoch 4/5 {'Train loss': '0.32277', 'Train accu': '88.94'}\n",
      "Validation... {'Dev loss': '0.40815', 'Dev accu': '82.29'}\n",
      "\n",
      "Epoch 5/5 {'Train loss': '0.19674', 'Train accu': '94.87'}\n",
      "Validation... {'Dev loss': '0.36995', 'Dev accu': '83.99'}\n",
      "\n",
      "CPU times: user 1min 22s, sys: 12 s, total: 1min 34s\n",
      "Wall time: 1min 17s\n"
     ]
    }
   ],
   "source": [
    "from pytorch_models.CNN import CNN\n",
    "\n",
    "model = CNN(len(V.vocab_to_idx), 2)\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "PT = PyTorchUtils(model, optimizer, criterion, include_seq_len=False)\n",
    "%time PT.train(train_loader, dev_loader, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3754351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Test loss': '0.33586', 'Test accu': '85.72'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PT.evaluate(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ecf331",
   "metadata": {},
   "source": [
    "<a name=\"8-3\"></a>\n",
    "### Test Two: RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa25bbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchify_fn = get_batchify_fn(V, include_seq_len=True)\n",
    "train_loader = create_dataloader(train_set, batchify_fn)\n",
    "dev_loader = create_dataloader(dev_set, batchify_fn, shuffle=False)\n",
    "test_loader = create_dataloader(test_set, batchify_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ae1407a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 {'Train loss': '0.68665', 'Train accu': '44.05'}\n",
      "Validation... {'Dev loss': '0.67081', 'Dev accu': '53.87'}\n",
      "\n",
      "Epoch 2/5 {'Train loss': '0.65342', 'Train accu': '59.90'}\n",
      "Validation... {'Dev loss': '0.64783', 'Dev accu': '61.24'}\n",
      "\n",
      "Epoch 3/5 {'Train loss': '0.60613', 'Train accu': '66.99'}\n",
      "Validation... {'Dev loss': '0.58531', 'Dev accu': '69.98'}\n",
      "\n",
      "Epoch 4/5 {'Train loss': '0.55404', 'Train accu': '71.38'}\n",
      "Validation... {'Dev loss': '0.57183', 'Dev accu': '71.03'}\n",
      "\n",
      "Epoch 5/5 {'Train loss': '0.49778', 'Train accu': '75.72'}\n",
      "Validation... {'Dev loss': '0.60941', 'Dev accu': '67.79'}\n",
      "\n",
      "CPU times: user 2min 2s, sys: 20.6 s, total: 2min 23s\n",
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "from pytorch_models.S_RNN import SimpleRNN\n",
    "\n",
    "model = SimpleRNN(len(V.vocab_to_idx), 2)\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "PT = PyTorchUtils(model, optimizer, criterion, include_seq_len=True)\n",
    "%time PT.train(train_loader, dev_loader, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a908a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Test loss': '0.61738', 'Test accu': '66.42'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PT.evaluate(test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
