{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bb04af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Zhengxiang (Jack) Wang \n",
    "# Date: 2022-01-19\n",
    "# GitHub: https://github.com/jaaack-wang "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddd2c37",
   "metadata": {},
   "source": [
    "## Get PyTorch\n",
    "\n",
    "In case you have not installed PyTorch,run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c92cb1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150aa034",
   "metadata": {},
   "source": [
    "## Preprocess and numericalize text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "741b52bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/w9/d_nplhzj4qx35xxlgljgdtjh0000gn/T/jieba.cache\n",
      "Loading model cost 1.043 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two vocabulary dictionaries have been built!\n",
      "Please call \u001b[1mX.vocab_to_idx | X.idx_to_vocab\u001b[0m to find out more where [X] stands for the name you used for this TextVectorizer class.\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "import jieba  # ---> tokenizer for Chinese\n",
    "\n",
    "# ---- load dataset ----\n",
    "train, dev, test = load_dataset(['train.tsv', 'dev.tsv', 'test.tsv'])\n",
    "\n",
    "# ---- numericalize the train set ----\n",
    "V = TextVectorizer(jieba.lcut) \n",
    "text = gather_text(train) # for collecting texts from train set\n",
    "V.build_vocab(text) # for building mapping vocab_to_idx dictionary and text_encoder\n",
    "\n",
    "train_encoded = list(encode_dataset(train, encoder=V)) # encodoing train set\n",
    "dev_encoded = list(encode_dataset(dev, encoder=V)) # encodoing dev set for validation\n",
    "test_encoded  = list(encode_dataset(test, encoder=V)) # encodoing dev set for prediction\n",
    "\n",
    "# ---- build mini batches for the train and dev set ----\n",
    "train_batched = build_batches(train_encoded, batch_size=64, \n",
    "                              max_seq_len=128, include_seq_len=False)\n",
    "\n",
    "dev_batched = build_batches(dev_encoded, batch_size=64, \n",
    "                            max_seq_len=128, include_seq_len=False)\n",
    "\n",
    "test_batched = build_batches(test_encoded, batch_size=64, \n",
    "                             max_seq_len=128, include_seq_len=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a76e3d1",
   "metadata": {},
   "source": [
    "### Convert numpy arrays into tensors\n",
    "\n",
    "It turns out that pytorch models do not accept numpy arrays during model training. The problem seems to be an attribute associated with `torch.Tensor` that has been named differently in `numpy.ndarray`, unlike `paddle`. \n",
    "\n",
    "To maintain consistency, this tutorial decided to not change the functions we will build together in the later tutorials. A better way of using packages in the pytorch ecosystem to preprocess and numericalize text data will be introduced separately, just as what I intended to do for the other two deep learning frameworks.\n",
    "\n",
    "Likewise, `PyTorchUtils` is also a wrapped up class I wrote up just to get this quick starts going, which will also be introduced later. Although this is not the best practice of using `pytorch`, you will find it useful when realizing the very nuanced differences between different deep learning frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "696236b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_utils import to_tensor\n",
    "\n",
    "train_batched = to_tensor(train_batched)\n",
    "dev_batched = to_tensor(dev_batched)\n",
    "test_batched = to_tensor(test_batched)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1052945f",
   "metadata": {},
   "source": [
    "## Training and evaluating models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0a72668",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_utils import PyTorchUtils\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb18d597",
   "metadata": {},
   "source": [
    "### BoW (Bag of Words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4376e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 {'Train loss': '0.83154', 'Train accu': '33.28'}\n",
      "Validation... {'Dev loss': '0.75056', 'Dev accu': '45.07'}\n",
      "\n",
      "Epoch 2/5 {'Train loss': '0.63411', 'Train accu': '51.41'}\n",
      "Validation... {'Dev loss': '0.70763', 'Dev accu': '51.34'}\n",
      "\n",
      "Epoch 3/5 {'Train loss': '0.55664', 'Train accu': '60.86'}\n",
      "Validation... {'Dev loss': '0.69626', 'Dev accu': '56.61'}\n",
      "\n",
      "Epoch 4/5 {'Train loss': '0.49809', 'Train accu': '68.38'}\n",
      "Validation... {'Dev loss': '0.69232', 'Dev accu': '60.88'}\n",
      "\n",
      "Epoch 5/5 {'Train loss': '0.44770', 'Train accu': '73.71'}\n",
      "Validation... {'Dev loss': '0.69498', 'Dev accu': '63.65'}\n",
      "\n",
      "CPU times: user 10.4 s, sys: 1.3 s, total: 11.7 s\n",
      "Wall time: 5.1 s\n"
     ]
    }
   ],
   "source": [
    "from pytorch_models.BoW import BoW\n",
    "\n",
    "\n",
    "model = BoW(len(V.vocab_to_idx), 2)\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "PT = PyTorchUtils(model, optimizer, criterion, include_seq_len=False)\n",
    "%time PT.train(train_batched, dev_batched, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed45b7ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Test loss': '0.64784', 'Test accu': '65.68'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PT.evaluate(test_batched)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974c471c",
   "metadata": {},
   "source": [
    "### CNN (Convolutional Neural Network) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf5de7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 {'Train loss': '0.67456', 'Train accu': '46.83'}\n",
      "Validation... {'Dev loss': '0.65148', 'Dev accu': '53.65'}\n",
      "\n",
      "Epoch 2/5 {'Train loss': '0.55648', 'Train accu': '71.35'}\n",
      "Validation... {'Dev loss': '0.52256', 'Dev accu': '72.64'}\n",
      "\n",
      "Epoch 3/5 {'Train loss': '0.40462', 'Train accu': '82.96'}\n",
      "Validation... {'Dev loss': '0.43706', 'Dev accu': '79.55'}\n",
      "\n",
      "Epoch 4/5 {'Train loss': '0.25230', 'Train accu': '92.81'}\n",
      "Validation... {'Dev loss': '0.39443', 'Dev accu': '82.43'}\n",
      "\n",
      "Epoch 5/5 {'Train loss': '0.13678', 'Train accu': '97.42'}\n",
      "Validation... {'Dev loss': '0.46483', 'Dev accu': '78.40'}\n",
      "\n",
      "CPU times: user 40.1 s, sys: 3.53 s, total: 43.6 s\n",
      "Wall time: 32.7 s\n"
     ]
    }
   ],
   "source": [
    "from pytorch_models.CNN import CNN\n",
    "\n",
    "\n",
    "model = CNN(len(V.vocab_to_idx), 2)\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "PT = PyTorchUtils(model, optimizer, criterion, include_seq_len=False)\n",
    "%time PT.train(train_batched, dev_batched, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1838b0db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Test loss': '0.43934', 'Test accu': '80.07'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PT.evaluate(test_batched)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9285af7c",
   "metadata": {},
   "source": [
    "## RNN (Recurrent neural network) \n",
    "\n",
    "As the RNN models also take as an input the sequence length, we need to re-encode the train set, dev set, and test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8764b280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- build mini batches for the train and dev set ----\n",
    "train_batched = build_batches(train_encoded, batch_size=64, \n",
    "                              max_seq_len=128, include_seq_len=True)\n",
    "\n",
    "dev_batched = build_batches(dev_encoded, batch_size=64, \n",
    "                            max_seq_len=128, include_seq_len=True)\n",
    "\n",
    "test_batched = build_batches(test_encoded, batch_size=64, \n",
    "                             max_seq_len=128, include_seq_len=True)\n",
    "\n",
    "train_batched = to_tensor(train_batched)\n",
    "dev_batched = to_tensor(dev_batched)\n",
    "test_batched = to_tensor(test_batched)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05d14c7",
   "metadata": {},
   "source": [
    "### Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7706134f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 {'Train loss': '0.69202', 'Train accu': '45.34'}\n",
      "Validation... {'Dev loss': '0.69197', 'Dev accu': '49.12'}\n",
      "\n",
      "Epoch 2/5 {'Train loss': '0.68519', 'Train accu': '51.04'}\n",
      "Validation... {'Dev loss': '0.69384', 'Dev accu': '50.85'}\n",
      "\n",
      "Epoch 3/5 {'Train loss': '0.67558', 'Train accu': '53.00'}\n",
      "Validation... {'Dev loss': '0.66694', 'Dev accu': '61.35'}\n",
      "\n",
      "Epoch 4/5 {'Train loss': '0.67112', 'Train accu': '54.17'}\n",
      "Validation... {'Dev loss': '0.71120', 'Dev accu': '50.55'}\n",
      "\n",
      "Epoch 5/5 {'Train loss': '0.66367', 'Train accu': '54.56'}\n",
      "Validation... {'Dev loss': '0.69764', 'Dev accu': '49.67'}\n",
      "\n",
      "CPU times: user 1min 53s, sys: 8.53 s, total: 2min 2s\n",
      "Wall time: 58.4 s\n"
     ]
    }
   ],
   "source": [
    "from pytorch_models.S_RNN import SimpleRNN\n",
    "\n",
    "\n",
    "model = SimpleRNN(len(V.vocab_to_idx), 2, bidirectional=False)\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "PT = PyTorchUtils(model, optimizer, criterion, include_seq_len=True)\n",
    "%time PT.train(train_batched, dev_batched, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "001e85d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Test loss': '0.68780', 'Test accu': '51.75'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PT.evaluate(test_batched)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4d9863",
   "metadata": {},
   "source": [
    "### LSTM (Long short-term memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11223765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 {'Train loss': '0.69281', 'Train accu': '42.83'}\n",
      "Validation... {'Dev loss': '0.69248', 'Dev accu': '49.70'}\n",
      "\n",
      "Epoch 2/5 {'Train loss': '0.68699', 'Train accu': '53.17'}\n",
      "Validation... {'Dev loss': '0.69406', 'Dev accu': '50.69'}\n",
      "\n",
      "Epoch 3/5 {'Train loss': '0.67005', 'Train accu': '52.60'}\n",
      "Validation... {'Dev loss': '0.68168', 'Dev accu': '56.88'}\n",
      "\n",
      "Epoch 4/5 {'Train loss': '0.63780', 'Train accu': '60.52'}\n",
      "Validation... {'Dev loss': '0.64299', 'Dev accu': '64.20'}\n",
      "\n",
      "Epoch 5/5 {'Train loss': '0.61705', 'Train accu': '62.87'}\n",
      "Validation... {'Dev loss': '0.64244', 'Dev accu': '63.82'}\n",
      "\n",
      "CPU times: user 8min 11s, sys: 2min 9s, total: 10min 20s\n",
      "Wall time: 5min 2s\n"
     ]
    }
   ],
   "source": [
    "from pytorch_models.LSTM import LSTM\n",
    "\n",
    "\n",
    "model = LSTM(len(V.vocab_to_idx), 2, bidirectional=False)\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "PT = PyTorchUtils(model, optimizer, criterion, include_seq_len=True)\n",
    "%time PT.train(train_batched, dev_batched, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "263dc104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Test loss': '0.64369', 'Test accu': '61.57'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PT.evaluate(test_batched)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f0f869",
   "metadata": {},
   "source": [
    "### GRU (Gated recurrent units)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "845b36b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 {'Train loss': '0.69216', 'Train accu': '42.46'}\n",
      "Validation... {'Dev loss': '0.69208', 'Dev accu': '50.27'}\n",
      "\n",
      "Epoch 2/5 {'Train loss': '0.68622', 'Train accu': '53.30'}\n",
      "Validation... {'Dev loss': '0.69203', 'Dev accu': '50.27'}\n",
      "\n",
      "Epoch 3/5 {'Train loss': '0.66835', 'Train accu': '56.89'}\n",
      "Validation... {'Dev loss': '0.64390', 'Dev accu': '64.17'}\n",
      "\n",
      "Epoch 4/5 {'Train loss': '0.62423', 'Train accu': '62.10'}\n",
      "Validation... {'Dev loss': '0.64492', 'Dev accu': '63.65'}\n",
      "\n",
      "Epoch 5/5 {'Train loss': '0.60530', 'Train accu': '63.14'}\n",
      "Validation... {'Dev loss': '0.64059', 'Dev accu': '64.09'}\n",
      "\n",
      "CPU times: user 6min 4s, sys: 59.4 s, total: 7min 4s\n",
      "Wall time: 3min 11s\n"
     ]
    }
   ],
   "source": [
    "from pytorch_models.GRU import GRU\n",
    "\n",
    "\n",
    "model = GRU(len(V.vocab_to_idx), 2, bidirectional=False)\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "PT = PyTorchUtils(model, optimizer, criterion, include_seq_len=True)\n",
    "%time PT.train(train_batched, dev_batched, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce7a2b5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Test loss': '0.64588', 'Test accu': '61.35'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PT.evaluate(test_batched)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
