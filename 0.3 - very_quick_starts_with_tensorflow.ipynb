{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d66c03d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Zhengxiang (Jack) Wang \n",
    "# Date: 2022-01-19\n",
    "# GitHub: https://github.com/jaaack-wang "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8f1d58",
   "metadata": {},
   "source": [
    "## Get TensorFlow\n",
    "\n",
    "In case you have not installed TensorFlow, run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "158e3770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Requires the latest pip\n",
    "# !pip3 install --upgrade pip\n",
    "# !pip3 install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c36182d",
   "metadata": {},
   "source": [
    "## Preprocess and numericalize text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21ecb647",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/w9/d_nplhzj4qx35xxlgljgdtjh0000gn/T/jieba.cache\n",
      "Loading model cost 0.705 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two vocabulary dictionaries have been built!\n",
      "Please call \u001b[1mX.vocab_to_idx | X.idx_to_vocab\u001b[0m to find out more where [X] stands for the name you used for this TextVectorizer class.\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "import jieba  # ---> tokenizer for Chinese\n",
    "\n",
    "# ---- load dataset ----\n",
    "train, dev, test = load_dataset(['train.tsv', 'dev.tsv', 'test.tsv'])\n",
    "\n",
    "# ---- numericalize the train set ----\n",
    "V = TextVectorizer(jieba.lcut) \n",
    "text = gather_text(train) # for collecting texts from train set\n",
    "V.build_vocab(text) # for building mapping vocab_to_idx dictionary and text_encoder\n",
    "\n",
    "train_encoded = list(encode_dataset(train, encoder=V)) # encodoing train set\n",
    "dev_encoded = list(encode_dataset(dev, encoder=V)) # encodoing dev set for validation\n",
    "test_encoded  = list(encode_dataset(test, encoder=V)) # encodoing dev set for prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082083a9",
   "metadata": {},
   "source": [
    "### A note\n",
    "\n",
    "There are multiple ways to use tensorflow to train a model, but the easiest one is to employ the `fit` method. In this `fit` function, the `inputs` and `targets` (labels) from the train set should be separately provided, and the `inputs` and `targets` from the dev set should be put inside a list or tuple. And both the `inputs` and `targets` should not be batched, as there is another builtin parameter called `batch_size` that will create mini batches for us. Nevertheless, to maintain consistency, this tutorial decided to still use the `build_batches` function that we will build together later in other tutorials. This `build_batches` function will help normalize the text seq length, which tensorflow's `fit` method does not provide. \n",
    "\n",
    "A better way of using packages in the tensorflow ecosystem to preprocess and numericalize text data will be introduced separately, just as what I intended to do for the other two deep learning frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dee221e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- build mini batches for the train and dev set ----\n",
    "train_batched = build_batches(train_encoded, batch_size=10000, \n",
    "                              max_seq_len=128, include_seq_len=False)\n",
    "\n",
    "dev_batched = build_batches(dev_encoded, batch_size=10000, \n",
    "                            max_seq_len=128, include_seq_len=False)\n",
    "\n",
    "test_batched = build_batches(test_encoded, batch_size=10000, \n",
    "                             max_seq_len=128, include_seq_len=False)\n",
    "\n",
    "train_X, train_Y = train_batched[0]\n",
    "dev_X, dev_Y = dev_batched[0]\n",
    "test_X, test_Y = test_batched[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43526e6",
   "metadata": {},
   "source": [
    "## Training and evaluating models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0456585",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189599b8",
   "metadata": {},
   "source": [
    "## BoW (Bag of Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d82063f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-19 23:56:48.788658: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-01-19 23:56:49.143742: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "63/63 [==============================] - 2s 19ms/step - loss: 0.5528 - accuracy: 0.7318 - val_loss: 0.4173 - val_accuracy: 0.8417\n",
      "Epoch 2/5\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.2875 - accuracy: 0.9160 - val_loss: 0.3348 - val_accuracy: 0.8750\n",
      "Epoch 3/5\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.1703 - accuracy: 0.9555 - val_loss: 0.2959 - val_accuracy: 0.8783\n",
      "Epoch 4/5\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.1116 - accuracy: 0.9753 - val_loss: 0.2815 - val_accuracy: 0.8800\n",
      "Epoch 5/5\n",
      "63/63 [==============================] - 1s 17ms/step - loss: 0.0819 - accuracy: 0.9837 - val_loss: 0.3094 - val_accuracy: 0.8725\n",
      "CPU times: user 17.4 s, sys: 2.64 s, total: 20 s\n",
      "Wall time: 6.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcc1ed45970>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tf_models.BoW import BoW\n",
    "\n",
    "\n",
    "model = BoW(len(V.vocab_to_idx), 1)\n",
    "model.compile(optimizer=keras.optimizers.Adam(5e-4),\n",
    "              loss=keras.losses.BinaryCrossentropy(),\n",
    "              metrics=[[\"accuracy\"]])\n",
    "\n",
    "%time model.fit(train_X, train_Y, epochs=5, batch_size=64, validation_data=(dev_X, dev_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cf40ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2872 - accuracy: 0.8700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.28718623518943787, 0.8700000047683716]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_X, test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c1ac14",
   "metadata": {},
   "source": [
    "## CNN (Convolutional Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "253bf7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_models.CNN import CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06dc6b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "63/63 [==============================] - 3s 44ms/step - loss: 0.6816 - accuracy: 0.5950 - val_loss: 0.6487 - val_accuracy: 0.7708\n",
      "Epoch 2/5\n",
      "63/63 [==============================] - 3s 42ms/step - loss: 0.5471 - accuracy: 0.7840 - val_loss: 0.4776 - val_accuracy: 0.8125\n",
      "Epoch 3/5\n",
      "63/63 [==============================] - 3s 42ms/step - loss: 0.3323 - accuracy: 0.8903 - val_loss: 0.3393 - val_accuracy: 0.8533\n",
      "Epoch 4/5\n",
      "63/63 [==============================] - 3s 42ms/step - loss: 0.1570 - accuracy: 0.9565 - val_loss: 0.2860 - val_accuracy: 0.8867\n",
      "Epoch 5/5\n",
      "63/63 [==============================] - 3s 43ms/step - loss: 0.0660 - accuracy: 0.9877 - val_loss: 0.2893 - val_accuracy: 0.8833\n",
      "CPU times: user 49.6 s, sys: 14.5 s, total: 1min 4s\n",
      "Wall time: 14 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcc204f7d00>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CNN(len(V.vocab_to_idx), 1)\n",
    "model.compile(optimizer=keras.optimizers.Adam(5e-4),\n",
    "              loss=keras.losses.BinaryCrossentropy(),\n",
    "              metrics=[[\"accuracy\"]])\n",
    "\n",
    "%time model.fit(train_X, train_Y, epochs=5, batch_size=64, validation_data=(dev_X, dev_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6662c522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2617 - accuracy: 0.8950\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2617317736148834, 0.8949999809265137]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_X, test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5be71b6",
   "metadata": {},
   "source": [
    "## RNN (Recurrent neural network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fae1917",
   "metadata": {},
   "source": [
    "## SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a8982c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_models.S_RNN import SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb94e7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "63/63 [==============================] - 5s 65ms/step - loss: 0.6819 - accuracy: 0.5598 - val_loss: 0.6753 - val_accuracy: 0.5842\n",
      "Epoch 2/5\n",
      "63/63 [==============================] - 4s 59ms/step - loss: 0.5229 - accuracy: 0.7610 - val_loss: 0.4445 - val_accuracy: 0.7967\n",
      "Epoch 3/5\n",
      "63/63 [==============================] - 4s 60ms/step - loss: 0.2644 - accuracy: 0.8953 - val_loss: 0.3497 - val_accuracy: 0.8517\n",
      "Epoch 4/5\n",
      "63/63 [==============================] - 4s 56ms/step - loss: 0.1012 - accuracy: 0.9695 - val_loss: 0.4123 - val_accuracy: 0.8492\n",
      "Epoch 5/5\n",
      "63/63 [==============================] - 4s 58ms/step - loss: 0.0534 - accuracy: 0.9885 - val_loss: 0.5034 - val_accuracy: 0.7883\n",
      "CPU times: user 58.4 s, sys: 18 s, total: 1min 16s\n",
      "Wall time: 19.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcc1fd531c0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SimpleRNN(len(V.vocab_to_idx), 1, bidirectional=False)\n",
    "model.compile(optimizer=keras.optimizers.Adam(5e-4),\n",
    "              loss=keras.losses.BinaryCrossentropy(),\n",
    "              metrics=[[\"accuracy\"]])\n",
    "\n",
    "%time model.fit(train_X, train_Y, epochs=5, batch_size=64, validation_data=(dev_X, dev_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "559135a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 7ms/step - loss: 0.4580 - accuracy: 0.8050\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4580153822898865, 0.8050000071525574]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_X, test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2fad34",
   "metadata": {},
   "source": [
    "## LSTM (Long short-term memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51e4e250",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_models.LSTM import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebaa36e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "63/63 [==============================] - 12s 147ms/step - loss: 0.6592 - accuracy: 0.6202 - val_loss: 0.5304 - val_accuracy: 0.8183\n",
      "Epoch 2/5\n",
      "63/63 [==============================] - 9s 137ms/step - loss: 0.4057 - accuracy: 0.8720 - val_loss: 0.3850 - val_accuracy: 0.8383\n",
      "Epoch 3/5\n",
      "63/63 [==============================] - 9s 136ms/step - loss: 0.2446 - accuracy: 0.9218 - val_loss: 0.3728 - val_accuracy: 0.8675\n",
      "Epoch 4/5\n",
      "63/63 [==============================] - 9s 136ms/step - loss: 0.1120 - accuracy: 0.9710 - val_loss: 0.3360 - val_accuracy: 0.8725\n",
      "Epoch 5/5\n",
      "63/63 [==============================] - 8s 133ms/step - loss: 0.0603 - accuracy: 0.9852 - val_loss: 0.6199 - val_accuracy: 0.8483\n",
      "CPU times: user 2min 13s, sys: 40.7 s, total: 2min 54s\n",
      "Wall time: 46.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcc0b7adb80>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTM(len(V.vocab_to_idx), 1, bidirectional=False)\n",
    "model.compile(optimizer=keras.optimizers.Adam(5e-4),\n",
    "              loss=keras.losses.BinaryCrossentropy(),\n",
    "              metrics=[[\"accuracy\"]])\n",
    "\n",
    "%time model.fit(train_X, train_Y, epochs=5, batch_size=64, validation_data=(dev_X, dev_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0846a7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 1s 19ms/step - loss: 0.5448 - accuracy: 0.8683\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.544813334941864, 0.8683333396911621]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_X, test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592f99b9",
   "metadata": {},
   "source": [
    "## GRU (Gated recurrent units) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3577037e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_models.GRU import GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb48ef52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "63/63 [==============================] - 10s 122ms/step - loss: 0.6837 - accuracy: 0.5690 - val_loss: 0.6377 - val_accuracy: 0.6700\n",
      "Epoch 2/5\n",
      "63/63 [==============================] - 7s 114ms/step - loss: 0.4196 - accuracy: 0.8190 - val_loss: 0.3533 - val_accuracy: 0.8558\n",
      "Epoch 3/5\n",
      "63/63 [==============================] - 7s 105ms/step - loss: 0.1532 - accuracy: 0.9503 - val_loss: 0.3640 - val_accuracy: 0.8542\n",
      "Epoch 4/5\n",
      "63/63 [==============================] - 7s 104ms/step - loss: 0.0647 - accuracy: 0.9835 - val_loss: 0.4736 - val_accuracy: 0.8567\n",
      "Epoch 5/5\n",
      "63/63 [==============================] - 7s 109ms/step - loss: 0.0378 - accuracy: 0.9923 - val_loss: 0.4410 - val_accuracy: 0.8525\n",
      "CPU times: user 1min 49s, sys: 34.4 s, total: 2min 23s\n",
      "Wall time: 37.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcc0dc31730>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GRU(len(V.vocab_to_idx), 1, bidirectional=False)\n",
    "model.compile(optimizer=keras.optimizers.Adam(5e-4),\n",
    "              loss=keras.losses.BinaryCrossentropy(),\n",
    "              metrics=[[\"accuracy\"]])\n",
    "\n",
    "%time model.fit(train_X, train_Y, epochs=5, batch_size=64, validation_data=(dev_X, dev_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4cc0daa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 12ms/step - loss: 0.3991 - accuracy: 0.8608\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3990863561630249, 0.8608333468437195]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_X, test_Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
