{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09fff211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Zhengxiang (Jack) Wang \n",
    "# Date: 2022-01-20, modified on 2022-01-22\n",
    "# GitHub: https://github.com/jaaack-wang "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408d3479",
   "metadata": {},
   "source": [
    "## Purposes of data preprocessing\n",
    "\n",
    "For natural lannguage processing, the ultimate purpose of data preprocessing is to convert text data into numerical data, so that mathematical operations can be run on the dataset inputted. This typically involves two general steps:\n",
    "\n",
    "- **Text tokenization**. Please note that, what I mean by tokenization here is not to simply tokenize a sequence of text into words. The results of text tokenization can be anything depending on what works best for you: whitespace-separated tokens, words, stems (subwords or words), lemma, or even characters (Yes, I do mean characters! Such as \"a, b, c, d...\" for English). Before tokenization, there may be a need of normalizing or standarizing the text, again depending on your needs. Text normalization may include: making all characters lower case, americanizing the spelling, removing extra spaces or stopwords etc.\n",
    "\n",
    "\n",
    "- **Text representation or numericalization**: This is simply to encode the tokenized text data into numerical data where every token (remember that a token can be anything, see above) is mapped to something numerical, which can either be a number or an array of numbers. For machine learning purposes, we usually convert a token into an array of numbers and there are two common approaches to it: one-hot encoding and word embedding. \n",
    "    <br><br>\n",
    "    - **One-hot encoding**. As suggested by the name, encodes every token into a sparse array that has as many elements as your vocabulary (unique token) size, where only the element whose index equal to the index of the token in your vocabulary will be 1 whereas the rest will be 0. For example, we have a vocabulary of 10,000 tokens. Say we have a token which happens to be \"token\" is the 4th word in the vocabulary. By one-hot encoding convention, the token \"token\" will be encoded as $(0, 0, 0, 1, 0, 0...0)$ where there are another 9996 zeros after 1.\n",
    "    <br><br>\n",
    "    - **Word embedding**. Word embedding is just a conventional name. In practice, we can embed any token the same way we embed words. The basic idea of word embedding is to convert words into a set of much denser arrays where every word (note that here word means token in the same sense as above) is mapped to a much shorter array. To make the array shorter, we need to employ continous floats instead of discrete integers so that a small difference in the floats can mean something. More about word embedding can be seen in the `README.md` file where I explain the general architecture of deep learning models used for text classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37d3f95",
   "metadata": {},
   "source": [
    "## What to do in the tutorial \n",
    "\n",
    "In this tutorial, we will do both text tokenization and text numericalization to the corpus we have compiled (see `1 - get_data.ipynb`). In addition, we will also learn to build mini batches on the top of the numericalized text data, which is a common practice in deep learning applications. \n",
    "\n",
    "Although most deep learning frameworks will provide handy functions for you to do text data processing that works more efficiently when using the frameworks to train models, it is important to understand the whole process so you will not get confused with what those provided functions do for you. Therefore, we will do everything from scratch here.  \n",
    "\n",
    "Specifically, we will \n",
    "\n",
    "- [first load the train set](#1)\n",
    "- [tokenize the text in the train set](#2)\n",
    "- [create dictionaries to map tokens to indices and vice versa](#3)\n",
    "- [numericalize the text data based on the mapping dictionaries](#4)\n",
    "- [and finally build mini batches for the numericalized text data](#5)\n",
    "\n",
    "Let's do it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8aff22b",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## Load the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "597e26d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(fpath, num_row_to_skip=1):\n",
    "    data = open(fpath)\n",
    "    for _ in range(num_row_to_skip):\n",
    "        next(data)\n",
    "    for line in data:\n",
    "        line = line.split('\\t')\n",
    "        yield line[1].rstrip(), int(line[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e902dd46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000,\n",
       " [('选择珠江花园的原因就是方便，有电动扶梯直接到达海边，周围餐馆、食廊、商场、超市、摊位一应俱全。酒店装修一般，但还算整洁。 泳池在大堂的屋顶，因此很小，不过女儿倒是喜欢。 包的早餐是西式的，还算丰富。 服务吗，一般',\n",
       "   1),\n",
       "  ('15.4寸笔记本的键盘确实爽，基本跟台式机差不多了，蛮喜欢数字小键盘，输数字特方便，样子也很美观，做工也相当不错', 1),\n",
       "  ('房间太小。其他的都一般。。。。。。。。。', 0)])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set = list(load_dataset('train.tsv'))\n",
    "\n",
    "# check\n",
    "len(train_set), train_set[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e507d4",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## Tokenize the text in the train set\n",
    "\n",
    "Unlike English, Chinese words are run together, which makes tokenizing them more difficult. Here we will use a popular Chinese text segmentation tool [`jieba`](https://github.com/fxsjy/jieba) to tokenize Chinese."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ed04ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "# define tokenize method\n",
    "tokenize = jieba.lcut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef1eece1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/w9/d_nplhzj4qx35xxlgljgdtjh0000gn/T/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  选择珠江花园的原因就是方便，有电动扶梯直接到达海边，周围餐馆、食廊、商场、超市、摊位一应俱全。酒店装修一般，但还算整洁。 泳池在大堂的屋顶，因此很小，不过女儿倒是喜欢。 包的早餐是西式的，还算丰富。 服务吗，一般\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.740 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized:  ['选择', '珠江', '花园', '的', '原因', '就是', '方便', '，', '有', '电动', '扶梯', '直接', '到达', '海边', '，', '周围', '餐馆', '、', '食廊', '、', '商场', '、', '超市', '、', '摊位', '一应俱全', '。', '酒店', '装修', '一般', '，', '但', '还', '算', '整洁', '。', ' ', '泳池', '在', '大堂', '的', '屋顶', '，', '因此', '很小', '，', '不过', '女儿', '倒', '是', '喜欢', '。', ' ', '包', '的', '早餐', '是', '西式', '的', '，', '还', '算', '丰富', '。', ' ', '服务', '吗', '，', '一般']\n"
     ]
    }
   ],
   "source": [
    "# let's see how it works\n",
    "\n",
    "text, _ = train_set[0]\n",
    "print(\"Original: \", text)\n",
    "print(\"Tokenized: \", tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace47a98",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a>\n",
    "## Create dictionaries to map tokens to indices and vice versa\n",
    "\n",
    "This includes a dictionary that maps tokens to indices (encoding) and a dictionary that maps indices to tokens (decoding). Creating such dictionaries is not very hard. All we need is just a vocabulary list (unique tokens) and then create a dictionary that assign every token with an unique number. Then we can reverse the vocabulary dictionary and get another dictionary where number is the key and token is the value. \n",
    "\n",
    "There are two ways to get a vobulary list: either create one internally from the text in the train set (but not dev set nor test set because we want to make them untouched), or obtain one externally. If you obtain a vocabulary list externaly, do make sure that the vocabulary list is compatible with the way how you tokenize text. Otherwise, it not very meaningful that the tokens obtained by your tokenizer can barely find unique indices in the vocabulary list. (For example, the vocabulary list is based on words, but your tokenizer tokenizes texts into characters...)\n",
    "\n",
    "Additionally, unless the vocabulary is definite or can be exclusively listed (characters-based), we will need a special token to map all unseen tokens that may occur from times to times. Usually, we denote such token as `[UNK]` or `<UNK>`. \n",
    "\n",
    "Furthermore, as the length varies from text to text, whereas deep learning models like to text squences of same length because eventually mathematical (specifically matrix) operations are particular about the dimensions of the data. For example, if two matrices have different dimensions, we cannot do element-wise addition or substraction (in programming world, however, there are certain cases where [`broadcasting`](https://numpy.org/doc/stable/user/basics.broadcasting.html) is allowed). More importanly is the matrix multiplication where two matrices must have compatible (not identical) dimensions such that multiplication can carry out. Nevertheless, a way to get around this is to pad all texts to a length so that every text (entirely or within a batch) is represented by an array of equal length. Typically, we use `[PAD]` or `<PAD>` to denote padded areas. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74b14c3",
   "metadata": {},
   "source": [
    "### All tokens in the train set\n",
    "\n",
    "To create a vocabulary dictionary, we need a list of unique tokens. Let's gather all tokens in the train set first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41494411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(274665, ['选择', '珠江', '花园', '的', '原因'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = []\n",
    "\n",
    "for (text, _) in train_set:\n",
    "    tokens += tokenize(text)\n",
    "\n",
    "# check\n",
    "len(tokens), tokens[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bfc173",
   "metadata": {},
   "source": [
    "### Unique tokens\n",
    "\n",
    "Then we need to gather all the unique tokens. This can be done either randomly or according to the frequnecy of the toknes' occurences (typically in a descending order). It is hard to tell which way is better (probably not so different), but when there is an overwhelming number of tokens in the vocabulary, it is easier to leave out low-frequency tokens out if we have already arranged the tokens in the vocabulary based on their occurences. A very large vocabulary will require additional computational resources, which sometimes we may want to avoid. \n",
    "\n",
    "However, in this tutorial, we will keep all the tokens in the vocabulary as there are only 22095 unique tokens from the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc94d290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 22095\n"
     ]
    }
   ],
   "source": [
    "# ramdonly \n",
    "unique_tokens = list(set(tokens))\n",
    "print(\"Number of unique tokens:\", len(unique_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ed4b0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 22095\n",
      "\n",
      "The 10 most frequent tokens in the train set and their occurrences:\n",
      "\n",
      "，         22012\n",
      "的         15152\n",
      "。         8613\n",
      "了         5654\n",
      "          4092\n",
      "是         3959\n",
      "我         3445\n",
      ",         3416\n",
      "很         2536\n",
      "！         2247\n"
     ]
    }
   ],
   "source": [
    "# based on occurences\n",
    "from collections import Counter\n",
    "\n",
    "unique_tks_counted = Counter(tokens).most_common()\n",
    "print(\"Number of unique tokens:\", len(unique_tks_counted))\n",
    "\n",
    "print(\"\\nThe 10 most frequent tokens in the train set and their occurrences:\\n\")\n",
    "tmp = \"{:10}{}\"\n",
    "for tk, count in unique_tks_counted[:10]:\n",
    "    print(tmp.format(tk, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6cccc4",
   "metadata": {},
   "source": [
    "### Finalize the vocabulary list\n",
    "\n",
    "You can add the two special tokens `[PAD]` and `[UNK]` here or later, depending on your preferences. In practice, it is typical that the `[PAD]` token is assigned with 0 and the `[UNK]` token is assigned with 1. Hence, tehy are placed as the first two items in the vocabulary list. \n",
    "\n",
    "Similarly, it is up to you to decide which way we want to order the rest tokens, either randomly or based on their occurences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f1987fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22097, ['[PAD]', '[UNK]', '，', '的', '。', '了', ' ', '是', '我', ','])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = ['[PAD]', '[UNK]'] + [tk for (tk, _) in unique_tks_counted]\n",
    "\n",
    "# check\n",
    "len(vocab), vocab[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc2dc03",
   "metadata": {},
   "source": [
    "### `vocab_to_idx` and `idx_to_vocab` dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8a20054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 10 items in the vocab_to_idx dicionary:\n",
      "\n",
      "[PAD]\t0\n",
      "[UNK]\t1\n",
      "，\t2\n",
      "的\t3\n",
      "。\t4\n",
      "了\t5\n",
      " \t6\n",
      "是\t7\n",
      "我\t8\n",
      ",\t9\n",
      "\n",
      "\n",
      "The first 10 items in the vocab_to_idx dicionary:\n",
      "\n",
      "0\t[PAD]\n",
      "1\t[UNK]\n",
      "2\t，\n",
      "3\t的\n",
      "4\t。\n",
      "5\t了\n",
      "6\t \n",
      "7\t是\n",
      "8\t我\n",
      "9\t,\n"
     ]
    }
   ],
   "source": [
    "vocab_to_idx = {tk: idx for idx, tk in enumerate(vocab)}\n",
    "idx_to_vocab = dict(map(reversed, vocab_to_idx.items()))\n",
    "\n",
    "# check\n",
    "tmp = \"{}\\t{}\"\n",
    "print(\"The first 10 items in the vocab_to_idx dicionary:\\n\")\n",
    "for tk, idx in list(vocab_to_idx.items())[:10]:\n",
    "    print(tmp.format(tk, idx))\n",
    "    \n",
    "print(\"\\n\\nThe first 10 items in the vocab_to_idx dicionary:\\n\")\n",
    "for idx, tk in list(idx_to_vocab.items())[:10]:\n",
    "    print(tmp.format(str(idx), tk))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a756bed1",
   "metadata": {},
   "source": [
    "### A note\n",
    "\n",
    "One thing to note here is that, as we want to map all the unseen vocabulary to the special token `[UNK]`, the `vocab_to_idx` dictionary needs to see all out-of-dictionary tokens as `[UNK]` and return the associated index (i.e., 1). We can do so by either by utilizing the `get` method that comes with the dictionary or the `defaultdict`. However, if you prefer the former, you need to create a lookup function for that accordingly. \n",
    "\n",
    "In this tutorial, we will use the `defaultdict` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3675acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index for 字: 491\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'nonexistent'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/w9/d_nplhzj4qx35xxlgljgdtjh0000gn/T/ipykernel_64549/2017918049.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# A problem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Index for 字:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_to_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'字'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Index for nonexistent:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_to_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nonexistent'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'nonexistent'"
     ]
    }
   ],
   "source": [
    "# A problem\n",
    "print(\"Index for 字:\", vocab_to_idx['字']) \n",
    "print(\"Index for nonexistent:\", vocab_to_idx['nonexistent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f91d9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index for 字: 491\n",
      "Index for nonexistent: 1\n"
     ]
    }
   ],
   "source": [
    "# get method and a look up function\n",
    "\n",
    "vocab_to_idx_lookup = lambda key: vocab_to_idx.get(key, 1)\n",
    "\n",
    "print(\"Index for 字:\", vocab_to_idx_lookup('字')) \n",
    "print(\"Index for nonexistent:\", vocab_to_idx_lookup('nonexistent'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f159b1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index for 字: 491\n",
      "Index for nonexistent: 1\n"
     ]
    }
   ],
   "source": [
    "# defaultdict\n",
    "from collections import defaultdict\n",
    "\n",
    "# \"lambda: 1\" lets the defaultdict return 1 when a key is not in the dict\n",
    "vocab_to_idx = defaultdict(lambda: 1, vocab_to_idx)\n",
    "\n",
    "print(\"Index for 字:\", vocab_to_idx['字']) \n",
    "print(\"Index for nonexistent:\", vocab_to_idx['nonexistent'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04f4e04",
   "metadata": {},
   "source": [
    "<a name=\"4\"></a>\n",
    "## Numericalize the text \n",
    "\n",
    "To numericalize the text is to encode it into a list of numbers. We will use the `vocab_to_idx` dictionary to do so. We will also create a decoder so that there is way we can decode the encoded text. We will use the `idx_to_vocab` dictionary to do this. \n",
    "\n",
    "As I mentioned earlier, the vocabulary list must be compatible with how our tokenizer tokenizes text. Only in this way can we use the `vocab_to_idx` and `idx_to_vocab` dictionaries to encode and decode text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86d7eef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_encoder(text, \n",
    "                 tokenize=tokenize, \n",
    "                 vocab_to_idx=vocab_to_idx):\n",
    "    \n",
    "    tokens = tokenize(text)\n",
    "    out = []\n",
    "    for tk in tokens:\n",
    "        out.append(vocab_to_idx[tk])\n",
    "    return out\n",
    "\n",
    "\n",
    "def text_decoder(text_ids, \n",
    "                 tokenize=tokenize, \n",
    "                 idx_to_vocab=idx_to_vocab, \n",
    "                 sep=\"\",\n",
    "                 out_str=True):\n",
    "    \n",
    "    out = []\n",
    "    for text_id in text_ids:\n",
    "        out.append(idx_to_vocab[text_id])\n",
    "    \n",
    "    if out_str:\n",
    "        return f\"{sep}\".join(out)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65133707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: 这只是一个简单的例子，看你懂不懂我\n",
      "\n",
      "Encoded: [24, 210, 34, 289, 3, 1622, 2, 28, 67, 1, 8]\n",
      "\n",
      "Decoded back: 这只是一个简单的例子，看你[UNK]我\n"
     ]
    }
   ],
   "source": [
    "# Am example\n",
    "\n",
    "text = \"这只是一个简单的例子，看你懂不懂我\"\n",
    "encoded_text = text_encoder(text)\n",
    "decoded = text_decoder(encoded_text)\n",
    "\n",
    "# note that all the punctuations are removed by the tokenizer. \n",
    "# \"Encoding, decoding, and Interestiiiiing\" are unseen tokens\n",
    "print(\"Original text:\", text)\n",
    "print(\"\\nEncoded:\", encoded_text)\n",
    "print(\"\\nDecoded back:\", decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99877237",
   "metadata": {},
   "source": [
    "### Encoding the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a37f2ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_dataset(dataset):\n",
    "    \n",
    "    for text, label in dataset:\n",
    "        text = text_encoder(text)\n",
    "        \n",
    "        yield [text, label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce28f5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Three encoded train set examples:\n",
      "\n",
      "Text\tLabel\n",
      "\n",
      "[188, 6965, 1177, 3, 428, 37, 99, 2, 17, 10681, 10682, 343, 913, 1875, 2, 604, 2299, 27, 10683, 27, 2133, 27, 1178, 27, 10684, 5392, 4, 13, 290, 76, 2, 44, 21, 259, 1048, 4, 6, 2685, 14, 284, 3, 4400, 2, 1089, 422, 2, 96, 397, 497, 7, 59, 4, 6, 531, 3, 108, 7, 2686, 3, 2, 21, 259, 471, 4, 6, 43, 319, 2, 76]\t1\n",
      "[3285, 959, 414, 3, 175, 304, 2134, 2, 305, 239, 1876, 639, 5, 2, 672, 59, 2488, 3733, 2, 6966, 2488, 2300, 99, 2, 605, 12, 10, 2687, 2, 344, 12, 383, 26]\t1\n",
      "[22, 504, 4, 163, 3, 16, 76, 4, 4, 4, 4, 4, 4, 4, 4, 4]\t0\n"
     ]
    }
   ],
   "source": [
    "train_set_encoded = list(encode_dataset(train_set))\n",
    "\n",
    "assert len(train_set) == len(train_set_encoded)\n",
    "\n",
    "tmp = \"{}\\t{}\"\n",
    "print(\"Three encoded train set examples:\\n\")\n",
    "print(tmp.format(\"Text\", \"Label\\n\"))\n",
    "\n",
    "for example in train_set_encoded[:3]:\n",
    "    print(tmp.format(*example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457c9b48",
   "metadata": {},
   "source": [
    "### Decoding the shown examples back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "edf9c11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Three decoded train set examples:\n",
      "\n",
      "Text\tLabel\n",
      "\n",
      "选择珠江花园的原因就是方便，有电动扶梯直接到达海边，周围餐馆、食廊、商场、超市、摊位一应俱全。酒店装修一般，但还算整洁。 泳池在大堂的屋顶，因此很小，不过女儿倒是喜欢。 包的早餐是西式的，还算丰富。 服务吗，一般\t1\n",
      "15.4寸笔记本的键盘确实爽，基本跟台式机差不多了，蛮喜欢数字小键盘，输数字特方便，样子也很美观，做工也相当不错\t1\n",
      "房间太小。其他的都一般。。。。。。。。。\t0\n"
     ]
    }
   ],
   "source": [
    "print(\"Three decoded train set examples:\\n\")\n",
    "print(tmp.format(\"Text\", \"Label\\n\"))\n",
    "\n",
    "for example in train_set_encoded[:3]:\n",
    "    t, l = example\n",
    "    decoded = text_decoder(t)\n",
    "    \n",
    "    print(tmp.format(decoded, l))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00ed2a7",
   "metadata": {},
   "source": [
    "<a name=\"5\"></a>\n",
    "## Build mini batches \n",
    "\n",
    "When building mini batches (more than one example for a batch) for the numericalized data, we need to make sure that the arrays in the batches have identical dimensions so that we can do matrix operations later on during the model training phase. Sometimes, for some tasks or for some models, we may need to ensure that every text is represented with a specified shape. For the neural network models represented in this repository, however, we only need to ensure the dimension for text_a or text_b is same within a given batch. Alternatively, we can set a `max_seq_len` that make every text represented by an arrary of `max_seq_len` items. A longer text with more tokens than the `max_seq_len` will be shortened accordingly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c42d376",
   "metadata": {},
   "source": [
    "### How to build batches \n",
    "\n",
    "First, let's see how we can create batches. Suppose we have a list of 50 items. If we want to create 10 batches, then each batch should have 5 items. Typically, we do it in a reverse way. That is, we specify how many items we want to have within each batch and then get the corresponding number of bacthes. Say we want to have 5 items each batch for the list, then we will need to create 10 batches. \n",
    "\n",
    "Logically, if we set a batch size that cannot divide the number of itmes in a list, then we need to make judgment call as to whether to drop the reminder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bcd34c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_creater(lst, \n",
    "                  batch_size, \n",
    "                  reminder_threshold=0):\n",
    "    \n",
    "    assert batch_size > 1, \"batch_size must be greater than 1\"\n",
    "    \n",
    "    lst_size = len(lst)\n",
    "    reminder = lst_size % batch_size\n",
    "    # if the threshold is greater than the reminder, \n",
    "    # we leave the reminder out. \n",
    "    if reminder_threshold > reminder:\n",
    "        lst = lst[:-reminder]\n",
    "        lst_size, reminder = len(lst), 0\n",
    "    \n",
    "    batch_num = lst_size // batch_size\n",
    "    # if there is a reminder, we need to add 1 to the batch_num\n",
    "    # so that the reminder can be included in the for loop\n",
    "    end_idx = batch_num + 1 if reminder else batch_num\n",
    "    \n",
    "    out = []\n",
    "    for i in range(0, end_idx):\n",
    "        out.append(lst[i * batch_size : (i+1) * batch_size])\n",
    "    \n",
    "    # if the reminder is 1, that means the last batch in the batch \n",
    "    # is one dimension less than the previous batch, which is listed. \n",
    "    if reminder == 1:\n",
    "        out[-1] = list(out[-1])\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c8162fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating batches. List size: 50. Batch size: 10...\n",
      "\n",
      "5 batches created!\n",
      "\n",
      "[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [10, 11, 12, 13, 14, 15, 16, 17, 18, 19], [20, 21, 22, 23, 24, 25, 26, 27, 28, 29], [30, 31, 32, 33, 34, 35, 36, 37, 38, 39], [40, 41, 42, 43, 44, 45, 46, 47, 48, 49]]\n"
     ]
    }
   ],
   "source": [
    "lst = list(range(50))\n",
    "print(\"Creating batches. List size: 50. Batch size: 10...\\n\")\n",
    "\n",
    "batches = batch_creater(lst, 10)\n",
    "print(f\"{len(batches)} batches created!\\n\")\n",
    "print(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b14cc6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating batches. List size: 50. Batch size: 10...\n",
      "\n",
      "8 batches created!\n",
      "\n",
      "[[0, 1, 2, 3, 4, 5, 6], [7, 8, 9, 10, 11, 12, 13], [14, 15, 16, 17, 18, 19, 20], [21, 22, 23, 24, 25, 26, 27], [28, 29, 30, 31, 32, 33, 34], [35, 36, 37, 38, 39, 40, 41], [42, 43, 44, 45, 46, 47, 48], [49]]\n"
     ]
    }
   ],
   "source": [
    "lst = list(range(50))\n",
    "print(\"Creating batches. List size: 50. Batch size: 10...\\n\")\n",
    "\n",
    "batches = batch_creater(lst, 7)\n",
    "print(f\"{len(batches)} batches created!\\n\")\n",
    "print(batches)\n",
    "\n",
    "# see the last batch with one item is also listed!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4840f6",
   "metadata": {},
   "source": [
    "### How to pad \n",
    "\n",
    "The key to pad is to make sure that every array in a list is of same length, so we need to decide: (1) when to pad; (2) what to pad with. For (1), we can pad when there are arrays in the list shorter than the longest one; that is, to make every array in the list is as long as the longest one. Or, we can set a `max_seq_len` and make every array to have the `max_seq_len` length no matter what. As for (2), we typically pad with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52415ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(lst, pad_idx=0, max_seq_len=None):\n",
    "    # here let's assume that every item in the lst is a list\n",
    "    \n",
    "    if max_seq_len:\n",
    "        max_len = max_seq_len\n",
    "    else:\n",
    "        max_len = max(len(l) for l in lst)\n",
    "    \n",
    "    lst_copy = lst.copy()\n",
    "    for idx, l in enumerate(lst_copy):\n",
    "        dif = max_len - len(l)\n",
    "        \n",
    "        # if there an item is shorter\n",
    "        if dif > 0:\n",
    "            lst_copy[idx] = lst_copy[idx] + [pad_idx] * dif\n",
    "        elif dif < 0:\n",
    "            lst_copy[idx] = lst_copy[idx][:max_len]\n",
    "    \n",
    "    return lst_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "adb36d99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4, 5, 0, 0],\n",
       " [1, 2, 3, 0, 0, 0, 0],\n",
       " [1, 2, 3, 4, 5, 6, 7],\n",
       " [1, 2, 3, 4, 0, 0, 0]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst = [\n",
    "    [1, 2, 3, 4, 5], \n",
    "    [1, 2, 3],\n",
    "    [1, 2, 3, 4, 5, 6, 7],\n",
    "    [1, 2, 3, 4]\n",
    "]\n",
    "\n",
    "pad(lst)\n",
    "\n",
    "# looks perfect!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84e966b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4, 5], [1, 2, 3, 0, 0], [1, 2, 3, 4, 5], [1, 2, 3, 4, 0]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad(lst, max_seq_len=5)\n",
    "\n",
    "# again, looks perfect!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36691528",
   "metadata": {},
   "source": [
    "### Let's build batches for our dataset!\n",
    "\n",
    "With everything put together, let's build batches for our dataset! \n",
    "\n",
    "Please note that: \n",
    "\n",
    "- in deep learning, we usually need to use `tensor` to represent a list of numbers, which makes training models easier and more efficiently especially on (parallel) GPUs. Here, we will use numpy array instead for illustration (numpy array is also trainable and convertible to tensor so no worries!).\n",
    "\n",
    "\n",
    "- we may also want to suffle the inputted dataset because the order of the dataset makes a difference. A good practice is to train models with shuffled train sets for multiple times and report the average performance scores. (To make things reproducible, a seed is also needed so that the shuffled train sets can be reproduced.)\n",
    "\n",
    "\n",
    "- whether to separate the labels from the texts does not matter as long as you know how to deal with both situations during the latter modelling training (as most deep learning frameworks are highly composable). So whatever works easiler for you. Here we will not separate them apart. \n",
    "\n",
    "\n",
    "- for different models, the required inputs can differ in order for the models to train, so **you need to build the `build_batches` function to suit your own needs**. In this repository, you will use Recurrent Neural Networks and its variants to train the text classifier, which all require the text length as inputs. The `build_batches` function is specific to this context and can actually be re-arranged into whatever way that works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "03f8fc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def build_batches(dataset, \n",
    "                  batch_size, \n",
    "                  shuffle=True, \n",
    "                  pad_idx=0, \n",
    "                  max_seq_len=None, \n",
    "                  dtype=\"int64\", \n",
    "                  reminder_threshold=0, \n",
    "                  include_seq_len=False):\n",
    "    \n",
    "    # ------------- building bacthes first -------------\n",
    "    \n",
    "    if shuffle:\n",
    "        np.random.shuffle(dataset)\n",
    "        \n",
    "    batches = batch_creater(dataset, batch_size, reminder_threshold)\n",
    "\n",
    "    \n",
    "    # ------------- start padding -------------\n",
    "    # we can reuse the pad func above but the following is more efficient\n",
    "    \n",
    "    def _pad(lst, max_len):\n",
    "        dif = max_len - len(lst)\n",
    "        if dif > 0:\n",
    "            return lst + [pad_idx] * dif\n",
    "        if dif < 0:\n",
    "            return lst[:max_len]\n",
    "        return lst\n",
    "        \n",
    "    \n",
    "    def pad(batch):\n",
    "        if max_seq_len:\n",
    "            max_len = max_seq_len\n",
    "        else:\n",
    "            max_len = max(len(b[0]) for b in batch)\n",
    "            \n",
    "        text, label = [], []\n",
    "        \n",
    "        if include_seq_len:\n",
    "            if max_seq_len:\n",
    "                text_len = [len(bt[0]) if len(bt[0]) < max_seq_len \n",
    "                              else max_seq_len for bt in batch]\n",
    "            else:\n",
    "                text_len = [len(bt[0]) for bt in batch]\n",
    "                \n",
    "            text_len = np.asarray(text_len, dtype=dtype)            \n",
    "        \n",
    "        for idx, bt in enumerate(batch):\n",
    "            \n",
    "            # ----- for text -----\n",
    "            text.append(_pad(bt[0], max_len))\n",
    "\n",
    "            # ----- for the label -----\n",
    "            label.append(bt[-1])\n",
    "        \n",
    "        text = np.asarray(text, dtype=dtype)\n",
    "        label = np.asarray(label, dtype=dtype)\n",
    "        \n",
    "        if include_seq_len:\n",
    "            return [text, text_len, label]\n",
    "        \n",
    "        return [text, label]\n",
    "    \n",
    "    out = []\n",
    "    for batch in batches:\n",
    "        out.append(pad(batch))\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83b3caa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 63 batches created!\n"
     ]
    }
   ],
   "source": [
    "train_set_batched = build_batches(train_set_encoded, batch_size=64, include_seq_len=True)\n",
    "\n",
    "print(f\"Number of {len(train_set_batched)} batches created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a18ee17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- # 1 batch ----------\n",
      "Shape of text batch: (64, 224)\n",
      "Shape of seq_len batch: (64,)\n",
      "Shape of label batch: (64,)\n",
      "\n",
      "---------- # 2 batch ----------\n",
      "Shape of text batch: (64, 706)\n",
      "Shape of seq_len batch: (64,)\n",
      "Shape of label batch: (64,)\n",
      "\n",
      "---------- # 3 batch ----------\n",
      "Shape of text batch: (64, 827)\n",
      "Shape of seq_len batch: (64,)\n",
      "Shape of label batch: (64,)\n",
      "\n",
      "---------- # 4 batch ----------\n",
      "Shape of text batch: (64, 309)\n",
      "Shape of seq_len batch: (64,)\n",
      "Shape of label batch: (64,)\n",
      "\n",
      "---------- # 5 batch ----------\n",
      "Shape of text batch: (64, 221)\n",
      "Shape of seq_len batch: (64,)\n",
      "Shape of label batch: (64,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check\n",
    "\n",
    "for idx, batch in enumerate(train_set_batched[:5]):\n",
    "    text, seq_len, label = batch\n",
    "    print(f\"{'-' * 10} # {idx+1} batch {'-' * 10}\")\n",
    "    \n",
    "    print(\"Shape of text batch:\", text.shape)\n",
    "    print(\"Shape of seq_len batch:\", seq_len.shape)\n",
    "    print(\"Shape of label batch:\", label.shape)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
