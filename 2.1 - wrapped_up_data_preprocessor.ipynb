{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "250b1a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Zhengxiang (Jack) Wang \n",
    "# Date: 2022-01-20\n",
    "# GitHub: https://github.com/jaaack-wang "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c0e463",
   "metadata": {},
   "source": [
    "## Quick start\n",
    "\n",
    "We used a lengthy tutorial to illustrate how to convert text dataset for training text classification models. However, if there are handy wrapped up functions, as available in many deep learning frameworks, this process can be easily done with a few lines of code. The following is an illustration, although in real projects, you many need additional lines of code to suit your specific needs. \n",
    "\n",
    "The following is a quick start. A more elaborated explanation is given afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bb7f78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/w9/d_nplhzj4qx35xxlgljgdtjh0000gn/T/jieba.cache\n",
      "Loading model cost 0.709 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two vocabulary dictionaries have been built!\n",
      "Please call \u001b[1mX.vocab_to_idx | X.idx_to_vocab\u001b[0m to find out more where [X] stands for the name you used for this TextVectorizer class.\n",
      "Number of 63 batches created!\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "import jieba\n",
    "\n",
    "# ---- load dataset ----\n",
    "train_set = load_dataset('train.tsv')\n",
    "\n",
    "# ---- numericalize the train set ----\n",
    "V = TextVectorizer(jieba.lcut) \n",
    "text = gather_text(train_set) # for collecting texts from train set\n",
    "V.build_vocab(text) # for building mapping vocab_to_idx dictionary and text_encoder\n",
    "train_set_encoded = list(encode_dataset(train_set, encoder=V.text_encoder)) # encodoing train set\n",
    "\n",
    "# ---- build mini batches for the train set ----\n",
    "train_set_batched = build_batches(train_set_encoded, batch_size=64, include_seq_len=True)\n",
    "print(f\"Number of {len(train_set_batched)} batches created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3341da",
   "metadata": {},
   "source": [
    "## Intro to the wrapped functions\n",
    "\n",
    "As we will not rewrite or copy and paste (a) same functions over and over again to just load and preprocess data throughout the tutorials, some wrapped functions are therefore provided. These wrapped functions can be seen in the `utils.py` file in the same folder.\n",
    "\n",
    "These wrapped functions mostly come from the last tutorial (see: `2 - preprocess_data.ipynb`) with some revisions to make some functons more reusable. Let's do what we do in the last tutorial and this tutorial will introduce these wrapped up functions along the way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3122b017",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "The wrapped up `load_dataset` allows:\n",
    "\n",
    "- loading a dataset (for tutorials in this repository) given its filepath; \n",
    "- (extended) loading multiples datasets given their filepathes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d0753d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 1200, 1200)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import load_dataset\n",
    "\n",
    "train_set, dev_set, test_set = load_dataset(['train.tsv', 'dev.tsv', 'test.tsv'])\n",
    "\n",
    "# check. should be 4000, 1200, 1200 (recall `1 - get_data.ipynb`)\n",
    "len(train_set), len(dev_set), len(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd087a6",
   "metadata": {},
   "source": [
    "## Numericalize text \n",
    "\n",
    "Recall that we need to encode the text data into something numerical so that we can train models on them. As elaborated in the last tutorial, to do so, we need to have a tokenizer and a related dictionary where we can map a token to an unique index. If we want to deocde the encoded text, we will also need to have a reversed dictionary that maps an index to a token. \n",
    "\n",
    "Re-doing all these can be tedious, so this tutorials introduces a highly wrapped up class function `TextVectorizer` to make everything easy. The `TextVectorizer` class can do the following: \n",
    "\n",
    "- building the `vocab_to_idx` and `idx_to_vocab` mapping dictionaries quickly given text and tokenizer. \n",
    "- encoding and decoding any given text(s) using the tokenizer used to build the dictionaries; \n",
    "- save and load the built `vocab_to_idx` and `idx_to_vocab` dictionaries for reuse; \n",
    "\n",
    "If you have a list of tokens ready that can be gotten from the tokenizer you pass to the `TextVectorizer` class, then you can also quickly build the dictionaries from that list of tokens. \n",
    "\n",
    "`TextVectorizer` is also callable, which means that after initializing it, you can directly encode text(s) by calling one(s) to it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f708d68",
   "metadata": {},
   "source": [
    "### Initialization \n",
    "\n",
    "To initialize the `TextVectorizer`, you must pass a tokenizer function/method to it, which takes str as input and returns a list of tokens. You can also pass a text preprocessor function/method to it to preprocess a given text before it is being tokenized by the tokenizer. Of course, you can build a tokenizer that incorprates the preprocessor and only pass the tokenizer. They are a same thing. \n",
    "\n",
    "We will use the same tokenizer we used in the last tutorial, which is saved in the `utils.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a63196d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import TextVectorizer\n",
    "import jieba\n",
    "\n",
    "\n",
    "V = TextVectorizer(jieba.lcut)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d6d47c",
   "metadata": {},
   "source": [
    "### Building `vocab_to_idx` and `idx_to_vocab` dictionaries\n",
    "\n",
    "To build these two dictionaries, all you need is just a (list) of text(s) and then pass it/them to the `TextVectorizer.build_vocab` function. If you have a list of tokens, you can also do so by passing the tokens to `TextVectorizer.build_vocab_from_list_tks`.\n",
    "\n",
    "If you choose to use `TextVectorizer.build_vocab`, you can also choose whether to build the vocab randomly or based on the occurences of the tokens in descending order. By defaults, it will build the vocab basde on the latter. When this is the case, you can specify how many most frequnt tokens you want to keep by specifying, for example, `top=10000`. This is not an option for the random mode because that does not make sense. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc12dbe4",
   "metadata": {},
   "source": [
    "To gather the text, we will use a simple function `gather_text` that specifically gather all the \"text_a\" and \"text_b\" from our datasets into a list. \n",
    "\n",
    "**Remember that the dictionaries should be built upon the train set or some external source, but never on the dev set or test set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e0229e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000 pieces of texts gathered from the train set.\n",
      "\n",
      "Two vocabulary dictionaries have been built!\n",
      "Please call \u001b[1mX.vocab_to_idx | X.idx_to_vocab\u001b[0m to find out more where [X] stands for the name you used for this TextVectorizer class.\n"
     ]
    }
   ],
   "source": [
    "from utils import gather_text\n",
    "\n",
    "text = gather_text(train_set)\n",
    "print(f\"{len(text)} pieces of texts gathered from the train set.\\n\")\n",
    "\n",
    "V.build_vocab(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7aebfdaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 random examples from the vocab_to_idx dictionary\n",
      "\n",
      "毛巾被                 21035\n",
      "微微                  4131\n",
      "冲突                  2966\n",
      "卡瓦                  15267\n",
      "端口                  19463\n",
      "\n",
      "\n",
      "5 random examples from the idx_to_vocab dictionary\n",
      "\n",
      "6066                差错\n",
      "9083                听不见\n",
      "2760                强悍\n",
      "18630               鸭绒被\n",
      "2321                套\n"
     ]
    }
   ],
   "source": [
    "from random import sample, seed\n",
    "# check\n",
    "\n",
    "seed(543)\n",
    "\n",
    "tmp = \"{:20}{}\"\n",
    "print(\"5 random examples from the vocab_to_idx dictionary\\n\")\n",
    "\n",
    "for item in sample(list(V.vocab_to_idx.items()), 5):\n",
    "    print(tmp.format(*item))\n",
    "    \n",
    "    \n",
    "print(\"\\n\\n5 random examples from the idx_to_vocab dictionary\\n\")\n",
    "\n",
    "for idx, tk in sample(list(V.idx_to_vocab.items()), 5):\n",
    "    print(tmp.format(str(idx), tk))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae17eb6c",
   "metadata": {},
   "source": [
    "### Saving the mapping dictionaries\n",
    "\n",
    "You can use `TextVectorizer.save_vocab_as_json` which by default will save the dictionaries, if built, as `vocab_to_idx.json` and `idx_to_vocab.json` in the current working directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c56568c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_to_idx.json has been successfully saved!\n",
      "idx_to_vocab.json has been successfully saved!\n"
     ]
    }
   ],
   "source": [
    "V.save_vocab_as_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea79823",
   "metadata": {},
   "source": [
    "### Reusing the mapping dictionaries\n",
    "\n",
    "When you need to reuse them, simply call `TextVectorizer.load_vocab_from_json`. If you do specify the filepathes to the two dictionaries, it will search for `vocab_to_idx.json` and `idx_to_vocab.json` in the current working directory and return them if any of them exists. \n",
    "\n",
    "Below, we first empty the two mapping dictionaries and reload them from the json files we saved just now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c30697b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_to_idx.json has been successfully loaded! Please call \u001b[1mX.vocab_to_idx\u001b[0m to find out more.\n",
      "idx_to_vocab.json has been successfully loaded! Please call \u001b[1mX.idx_to_vocab\u001b[0m to find out more.\n",
      "\n",
      "Where [X] stands for the name you used for this TextVectorizer class.\n"
     ]
    }
   ],
   "source": [
    "V.vocab_to_idx, V.idx_to_vocab = None, None\n",
    "\n",
    "V.load_vocab_from_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f23dd4",
   "metadata": {},
   "source": [
    "### Encoding and decoding text\n",
    "\n",
    "- Encoding: call initialized `TextVectorizer` or `TextVectorizer.text_encoder`.\n",
    "- Decoding: call `TextVectorizer.text_decoder`.\n",
    "\n",
    "Let's use some sample texts from the dev set as examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdce19d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 酒店的地理位置还算便利，但是酒店的设施非常成旧。前台一听说是携程订的，给的房间全部是陈旧的客房，设施很差，但房价很高。和当地朋友一说，他们认为价格比他们去订高多了！唉……\n",
      "Encoded: [13, 3, 478, 21, 259, 964, 2, 58, 13, 3, 133, 53, 17157, 4, 95, 147, 933, 7, 122, 373, 3, 2, 40, 3, 22, 1010, 7, 392, 3, 399, 2, 133, 467, 2, 44, 495, 10, 157, 4, 25, 1060, 131, 4880, 2, 139, 559, 1576, 139, 1, 1, 5, 11, 1246, 183, 183]\n",
      "Decoded: 酒店的地理位置还算便利，但是酒店的设施非常成旧。前台一听说是携程订的，给的房间全部是陈旧的客房，设施很差，但房价很高。和当地朋友一说，他们认为价格比他们[UNK][UNK]了！唉……\n",
      "\n",
      "Original: 有点厚重。显卡不是独立的，是集显，如果玩大型网游的，建议买独显的。\n",
      "Encoded: [75, 3544, 4, 449, 66, 1485, 3, 2, 7, 17327, 2, 111, 868, 3035, 1, 3, 2, 185, 38, 1570, 3, 4]\n",
      "Decoded: 有点厚重。显卡不是独立的，是集显，如果玩大型[UNK]的，建议买独显的。\n",
      "\n",
      "['酒店的地理位置还算便利，但是酒店的设施非常成旧。前台一听说是携程订的，给的房间全部是陈旧的客房，设施很差，但房价很高。和当地朋友一说，他们认为价格比他们[UNK][UNK]了！唉……', '有点厚重。显卡不是独立的，是集显，如果玩大型[UNK]的，建议买独显的。']\n"
     ]
    }
   ],
   "source": [
    "seed(894)\n",
    "\n",
    "dev_text = gather_text(dev_set)\n",
    "sample_texts = sample(dev_text, 2)\n",
    "\n",
    "assert len(dev_text) == len(dev_set)\n",
    "\n",
    "for text in sample_texts:\n",
    "    encoded = V.text_encoder(text) # or V(text)\n",
    "    decoded = V.text_decoder(encoded, sep='')\n",
    "    print(\"Original:\", text)\n",
    "    print(\"Encoded:\", encoded)\n",
    "    print(\"Decoded:\", decoded)\n",
    "    print()\n",
    "    \n",
    "# Or pass a list of text or text_ids to encode or decode\n",
    "print(V.text_decoder(V(sample_texts), sep=''))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fb0760",
   "metadata": {},
   "source": [
    "### Encode the train set\n",
    "\n",
    "We will reuse the `encode_dataset` built in the last tutorial to encode the train set (or dev set for validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76e3868d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[188, 6965, 1177, 3, 428, 37, 99, 2, 17, 10681, 10682, 343, 913, 1875, 2, 604, 2299, 27, 10683, 27, 2133, 27, 1178, 27, 10684, 5392, 4, 13, 290, 76, 2, 44, 21, 259, 1048, 4, 6, 2685, 14, 284, 3, 4400, 2, 1089, 422, 2, 96, 397, 497, 7, 59, 4, 6, 531, 3, 108, 7, 2686, 3, 2, 21, 259, 471, 4, 6, 43, 319, 2, 76], 1]\n"
     ]
    }
   ],
   "source": [
    "from utils import encode_dataset\n",
    "\n",
    "train_set_encoded = list(encode_dataset(train_set, encoder=V.text_encoder))\n",
    "\n",
    "# check\n",
    "print(train_set_encoded[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ea2774",
   "metadata": {},
   "source": [
    "## Building batches\n",
    "\n",
    "We will reuse the `build_batches` built in the last tutorial to build the mini batches for the train set (or dev set for validation). The parameters are same except one `include_seq_len` that is added specifically for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40ae7d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 63 batches created!\n"
     ]
    }
   ],
   "source": [
    "from utils import build_batches\n",
    "\n",
    "train_set_batched = build_batches(train_set_encoded, batch_size=64, include_seq_len=True)\n",
    "\n",
    "print(f\"Number of {len(train_set_batched)} batches created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c51c4e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- # 1 batch ----------\n",
      "Shape of text batch: (64, 325)\n",
      "Shape of seq_len batch: (64,)\n",
      "Shape of label batch: (64,)\n",
      "\n",
      "---------- # 2 batch ----------\n",
      "Shape of text batch: (64, 291)\n",
      "Shape of seq_len batch: (64,)\n",
      "Shape of label batch: (64,)\n",
      "\n",
      "---------- # 3 batch ----------\n",
      "Shape of text batch: (64, 163)\n",
      "Shape of seq_len batch: (64,)\n",
      "Shape of label batch: (64,)\n",
      "\n",
      "---------- # 4 batch ----------\n",
      "Shape of text batch: (64, 247)\n",
      "Shape of seq_len batch: (64,)\n",
      "Shape of label batch: (64,)\n",
      "\n",
      "---------- # 5 batch ----------\n",
      "Shape of text batch: (64, 334)\n",
      "Shape of seq_len batch: (64,)\n",
      "Shape of label batch: (64,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check\n",
    "\n",
    "for idx, batch in enumerate(train_set_batched[:5]):\n",
    "    text, seq_len, label = batch\n",
    "    print(f\"{'-' * 10} # {idx+1} batch {'-' * 10}\")\n",
    "    \n",
    "    print(\"Shape of text batch:\", text.shape)\n",
    "    print(\"Shape of seq_len batch:\", seq_len.shape)\n",
    "    print(\"Shape of label batch:\", label.shape)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
